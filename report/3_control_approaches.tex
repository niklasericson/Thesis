% !TEX root = main.tex
\chapter{Theory}\label{cha:controlApproach}
This chapter presents the motivation and the theory behind each of the control approaches investigated in this thesis. In the first two sections, an adaptive controller and an integral resonance controller is presented in detail. In the last and following section, harmonic cancellation is discussed, where three different approaches are described and motivated.

\section{Model Reference Adaptive Control}
An adaptive controller has the ability to adjust the system response by updating the parameters of a feedback controller in real time, resulting in a controller that is less sensitive to changes in the model and aging of the system. One approach is to use a reference model to create the desired system response which serves as a target for the adaptive laws. This approach is known as the Model Reference Adaptive Controller (\abbrMRAC). This model does not require any prior knowledge about the model uncertainties, implying a more straight-forward way to implement precision control to nanopositioning systems. Moreover, this scheme allows for the use of a lower order model (in relation to the system model) since the online parameter estimation can be used sufficiently with a lower order model. The \abbrMRAC scheme can be extended to include perturbation estimation (\abbrMRACPE), giving the controller the ability to compensate for various non-modeled effects, including both linear and nonlinear perturbations. Nonlinear effects such as the hysteresis are treated as lumped perturbations to the nominal system model and can be compensated for in the same manner as for linear disturbances, using the knowledge of the system and the previous measurement and output signal.

\subsection{Perturbation Estimation}\label{sec:pertest}
Using a second order model, the adaptive laws can be derived as follows. Consider the system model stated below.
\begin{equation}
  \label{eq:sysmodel}
  \ddot{x}(t) + \alpha_1\dot{x}(t) +  \alpha_0x(t) = \beta_0u(t) + f(t)
\end{equation}

where $x(t)$ denotes the output angle at time t, $u(t)$ the input voltage at time t and $\alpha_1, \alpha_0, \beta_0 \in \mathbb{R}$ are known system constants. $f(t)$ is a function describing the unknown perturbations of the system, including the hysteresis and creep effect. The general equations for deriving the perturbation function are described more thoroughly in~\citep{Elmali:1996}. For a simple second order SISO-model the perturbation estimation becomes

\begin{equation}
  \label{eq:perturbation}
  \hat{f}(t) = \ddot{x}_{cal}(t) + \alpha_1\dot{x}_{cal}(t) +  \alpha_0x(t) - \beta_0u(t-T_s)
\end{equation}

where $x_{cal}^{(n)}$ denotes the calculated state of the $n$th order of time derivative, $T_s$ is the sampling time interval and $u(t-T_s)$ is the control input in the previous time step. $u(t-T_s)$ is often approximated to $u(t)$ in practice, which is a valid approximation if $T_s$ is sufficiently small. Note that $x(t)$ here is the sensor input, i.e. the measured yaw angle.

Each state is, for its computational efficiency, computed by a simple backward different equation depicted below.

\begin{equation}
  \label{eq:backward}
  x_{cal}^{(n)}(t) = \frac{x_{cal}^{(n-1)}(t) - x_{cal}^{(n-1)}(t-T_s)}{T_s}
\end{equation}

\subsection{Adaptive Laws}
The objective of the adaptive laws is to calculate the control parameter so that they converge to ideal values resulting in a system response that matches the reference. The adaptive laws can be derived using Lyaponov theory which is outlined in this section. Consider the second order reference model below

\begin{equation}
  \label{eq:refmodel}
  \ddot{x}_m(t) + a_1\dot{x}_m(t) +  a_0x_m(t) = b_0u_d(t)
\end{equation}

where $x_m(t)$ denotes the output angle, $u_d(t)$ the input voltage and $a_0, a_1, b_0$ are known positive constants.

The tracking error is defined as below.
\begin{equation}
  \label{eq:stateerror}
  e(t) = x(t) - x_m(t)
\end{equation}

Recalling~\eqref{eq:sysmodel}, replacing $f(t)$ with the estimation $\hat{f}(t)$ and subtracting it from~\eqref{eq:refmodel} gives the following expression where more details can be found in~\citep{Qingson:2016}.

\begin{equation}
  \ddot{e}(t) + a_1\dot{e}(t) + a_0\dot{e}(t) =  (a_1-\alpha_1)\dot{x}(t) + (a_0-\alpha_0)x(t) - b_0u_d(t) - \beta_0u(t) + \hat{f}(t)
\end{equation}

Transforming it into state-space form
\begin{equation}
  \label{eq:stateSpaceError}
  \mathbf{\dot{E} = AE} + \beta_0\mathbf{B}u + \Delta
\end{equation}
where
\begin{equation}
  \label{eq:matrices}
  \mathbf{E} =
    \begin{bmatrix}
       e\\[0.3em]
       \dot{e}
     \end{bmatrix},
  \mathbf{A} =
    \begin{bmatrix}
       0 & 1\\[0.3em]
       -a_0 & -a_1
     \end{bmatrix},
  \mathbf{B} =
    \begin{bmatrix}
        0\\[0.3em]
        1
    \end{bmatrix},
    \mathbf{\Delta} =
      \begin{bmatrix}
          0\\[0.3em]
          \delta
      \end{bmatrix}
\end{equation}
with $\delta = (a_1-\alpha_1)\dot{x}(t) + (a_0-\alpha_0)x(t) - b_0u_d(t) + \hat{f}(t)$.

If all eigenvalues of $\mathbf{A}$ have negative real parts, then $\mathbf{E}$ will tend to zero as  $t \to \infty$, i.e. the system is asymptotically stable. Moreover, according to Lyapunov theory \citep{Ljung:2003}, for each positive-semidefinite matrix $\mathbf{Q}$ there exists one positive-semidefinite matrix $\mathbf{P}$ which solves \eqref{eq:lyap}.

\begin{equation}
  \label{eq:lyap}
  \mathbf{A^TP + PA = -Q}
\end{equation}

With the auxiliary item $\hat{e} = \mathbf{E^TPB}$, the adaptive laws are given by

\begin{equation}
  \label{eq:adaplaws}
  u = k_0u_d + k_1x + k_2\dot{x} + k_3\hat{f}
\end{equation}
and the control law parameters are calculated as outlined below where $\eta_i$ are tuning variables.
\begin{equation}
  \label{eq:adaplaws1}
  \dot{k}_0 = -\eta_0\hat{e}u_d
\end{equation}
\begin{equation}
  \label{eq:adaplaws2}
  \dot{k}_1 = -\eta_1\hat{e}x
\end{equation}
\begin{equation}
  \label{eq:adaplaws3}
  \dot{k}_2 = -\eta_2\hat{e}\dot{x}
\end{equation}
\begin{equation}
  \label{eq:adaplaws4}
  \dot{k}_3 = -\eta_3\hat{e}\hat{f}
\end{equation}

The proof can be found in \citep{Qingson:2016}. Substituting $\hat{f}$ in \eqref{eq:perturbation} with the one in \eqref{eq:adaplaws} and rearranging the parameters result in the final \abbrMRACPE control law which is stated below.

\begin{equation}
    \label{eq:adaplawsfinal}
  u(t) = k_0u_d(t) + (k_1 + k_3\alpha_0)x(t) +  (k_2 + k_3\alpha_1)\dot{x}(t) + k_3\ddot{x}(t) - k_3\beta_0u(t-T_s)
\end{equation}

A block diagram of the final controller, with inspiration from Figure 9.1 in ~\citep{Qingson:2016}, is depicted in Figure~\ref{fig:adaptive}. The adaptive controller consists of four blocks. One reference model that calculates the desired states $\mathbf{x_m}=[\dot{x}_m, x_m]^T$ from the input signal according to \eqref{eq:refmodel}, one adaptive mechanism that implements \eqref{eq:adaplaws1}-\eqref{eq:adaplaws4} and calculates $\mathbf{k}=[k_1, k_2, k_3, k_4]^T$, one state calculator that uses \eqref{eq:backward} to calculate $\mathbf{x}=[\ddot{x}, \dot{x}, x]^T$ and finally one controller block that uses \eqref{eq:adaplawsfinal} to calcuate the control signal $u$ that is sent to the rotational stage.

\begin{figure}[h]
  \centering %crop: left bottom right top
  \includegraphics[width=0.85\textwidth, trim=5cm 0cm 3.8cm 0cm, clip=true]{fig/matlab/adaptive_scheme}
  \caption{\label{fig:adaptive}Block diagram of the adaptive controller}
\end{figure}

\newpage~\newpage~
\FloatBarrier
\section{Integral Resonance Control}\label{sec:irc}
The integral resonance control (\abbrIRC) can be efficiently used to damp out the first resonant mode of the system, allowing for larger controller gains and a higher control bandwidth. The \abbrIRC block scheme is illustrated in Figure~\ref{fig:irc} and consists of a constant feed-through term $D_f<0$ and a negative integral controller $C(s)=\frac{-k}{s}$ where $k>0$ for stability. The negative feedforward term will, if sufficiently large and negative, introduce a pair of complex zeros below the first resonance frequency and ensure zero-pole cancellation for higher resonance modes as shown in \citep{Aphale:2007}. The addition of a negative feedforward will subtract, in the low frequency domain, a phase of $-180^{\circ}$. The phase margin can easily be increased by applying a simple negative integral controller to provide a 90 degrees phase lead.

The negative gain $D_f$ is straight-forward to manually select for introducing a complex pair of zeros below the first resonance. The integral gain $k$ can be chosen by using the root locus technique and selecting a gain that maximizes damping.

\begin{figure}[h]
  \centering %crop: left bottom right top
  \includegraphics[width=0.85\textwidth, trim=5.5cm 4cm 5.1cm 9.5cm, clip=true]{fig/matlab/irc}
  \caption{\label{fig:irc}Block diagram of the \abbrIRC damping loop.}
\end{figure}

The \abbrIRC scheme in Figure~\ref{fig:irc} can be simplified, by combining $C(s)$ and $D_f$ in the same block, the resulting scheme is shown in the inner loop in Figure~\ref{fig:irc_int}, where

\begin{equation}
  \label{eq:C2}
  C_2(s) = \frac{C(s)}{1+C(s)D_f}\Bigg|_{C(s) = \frac{-k}{s}} = \frac{-k}{s - kD_f}
\end{equation}

For tracking reference trajectories, the \abbrIRC can be enclosed in an outer loop, also seen in Figure~\ref{fig:irc_int}, utilizing a second controller $C_1(s)$ to compensate for disturbances and model errors \citep{gu:2014}. The closed loop system ($r$ to $y$) and the sensitivity function ($d_0$ to $y$) for the \abbrIRC is written below.

\begin{subequations}
  \label{eq:discrsys}
  \begin{alignat}{2}
    & G_c(s) = \frac{C_1(s)C_2(s)G(s)}{1 + C_2(s)G(s) + C_1(s)C_2(s)G(s)} \\
    & S(s) = \frac{1}{1 + C_2(s)G(s) + C_1(s)C_2(s)G(s)} \label{eq:discrsys1}
  \end{alignat}
\end{subequations}

\begin{figure}[h]
  \centering %crop: left bottom right top
  \includegraphics[width=0.95\textwidth, trim=4cm 5cm 3.6cm 9.5cm, clip=true]{fig/matlab/irc_int}
  \caption{\label{fig:irc_int}Block diagram of the tracking control system with \abbrIRC included.}
\end{figure}

Proof for the zero-pole entanglement and the insertion of the complex conjugate zeros can be found in \citep{Aphale:2007}, but note that the proof is only given for causal systems with a relative degree of two i.e two more poles than zeros.

To give the reader an intuitive explanation of the \abbrIRC and for a system with a relative degree of one, a brief example of a low order system is provided below. Let G be represented by a transfer function with a relative degree of one, with two poles and one zero as written below,

\begin{equation}
  \label{eq:irc_sys}
  G(s) = \frac{s + \alpha_0}{s^2 + \beta_1s + \beta_0}
\end{equation}

where  $\alpha_i > 0$ and  $\beta_i > 0$, i.e. a stable and minimum phase system.  Using $G_d(s) = G(s) + D_f$  ~\eqref{eq:irc_sys} and rearranging the terms gives

\begin{equation}
  \label{eq:irc_sys_d}
  \begin{split}
  G_d(s) & = \frac{s + \alpha_0}{s^2 + \beta_1s + \beta_0} + D_f \\
      & = \frac{D_fs^2 + (1 + D_f\beta_1)s + \alpha_0 + D_f\beta_0}{s^2 + \beta_1s + \beta_0} \\
      & = D_f\frac{s^2 + (\frac{1}{D_f} + \beta_1)s + \frac{\alpha_0}{D_f} + \beta_0}{s^2 + \beta_1s + \beta_0}
  \end{split}
\end{equation}


which illustrates that the number of introduced zeros is equal to the relative degree of the transfer function. Moreover, all zeros will have negative real part if the coefficients in $s^2 + (\frac{1}{D_f} + \beta_1)s + (\frac{\alpha_0}{D_f} + \beta_0)$  are larger than zero i.e. $ \frac{1}{D_f} + \beta_1>0$ and $\frac{\alpha_0}{D_f} + \beta_0>0$. This can be simplified to the conditions given in \eqref{eq:irc_cond}.

\begin{equation}
  \label{eq:irc_cond}
  \begin{cases}
    D_f < -\frac{1}{\beta_1}\\
    D_f < -\frac{\alpha_0}{\beta_0}\\
  \end{cases}
\end{equation}
\newpage

\section{Harmonic Cancellation}
Cancellation of specific harmonics can be utilized to increase the regulation capability of a controller. A known or estimated disturbance can in many cases be efficiently eliminated by a number of methods \citep{fujimoto2009rro, fujimoto2004repetitive, vilanova2008disturbance}. Many of these approaches are based on the Internal Model Principle (\abbrIMP) meaning that the controller incorporates a known model of the disturbance within the control loop itself. However, including the disturbance model for effective cancellation in the feedback loop will deteriorate the sensitivity function. Although the sensitivity function is zero for selected frequencies, it is increased for other nearby frequencies, leading to severe damage in the total tracking accuracy. This phenomenon can be explained by Bode's integral constraints \citep{Ljung:2003}. Hence, a feedforward approach is preferable to preserve the fine closed loop characteristics. For the sake of completeness, the \abbrIMP feedback approach is included in this chapter and evaluated in simulations to verify the expected results.

\subsection{Feedforward Disturbance Cancellation}\label{subsec:distff}
If a disturbance is measurable during operation, a feedforward of the disturbance model response can be used to eliminate the disturbance before it becomes present in the output signal \citep{industrial}. A simple block diagram of the structure is shown in Figure~\ref{fig:ffdist}, where $G, C, P_d$ and $K_f$ represent the system, the controller, the disturbance model and the feedforward block respectively.

\begin{figure}[h]
  \centering %crop: left bottom right top
  \includegraphics[width=0.8\textwidth, trim=8cm 4.5cm 5.97cm 8.5cm, clip=true]{fig/matlab/ffdist}
  \caption{\label{fig:ffdist}Block diagram of a control structure with feedforward from a known modeled disturbance.}
\end{figure}

The output is described by the following expression

\begin{equation}
  \label{eq:ffdist}
  Y(s) = \frac{C(s)G(s)}{1+C(s)G(s)}R(s) + \frac{P_d(s) - K_f(s)G(s)}{1+C(s)G(s)}D_0(s)
\end{equation}

and hence an ideal choice of $K_f(s)$ would be $K_f(s)=P_d(s)/G(s)$ which would eliminate the disturbance completely. It is worth noting that the ideal $K_f(s)$ might not be fully implementable (stable, proper and causal) and that the inverse of $G(s)$ has to be approximated, leading to merely partial cancellation of the disturbance. This approximation can still be sufficient if the inverse is constructed in a way so that $(P_d(s) - K_f(s)G(s))/(1+C(s)G(s))$ becomes small for the frequencies where the disturbance has the most impact on the system.

\subsection{Cancellation with Internal Model Principle}\label{subsec:distimp}
The \abbrIMP says that if a disturbance (entering the system on the output or input) can be described by a generating polynomial $\Gamma(s)$ then a standard one \abbrDOF-controller $C_{t}(s) = P(s)/(\Gamma(s)\bar{L}(s))$ can be used to asymptotically reject the effect of a modeled disturbance \citep{IMP:Perry}.  The generating polynomial $\Gamma(s) = f(0,s)/D(s)$, is derived by taking the Laplace transform of the differential equation describing the disturbance where $f(0,s)$ arises from non-zero initial conditions. To show the principle of \abbrIMP parts of the evidence derived in \citep{IMP:Perry} is presented here. Consider the system model $G(s) = B(s)/A(s)$. Using this system with the controller $C_t(s)$ above in closed loop yields the sensitivity function in \eqref{eq:s_imp}, which is the transfer function from output disturbance to output.
\begin{equation}
  \label{eq:s_imp}
  S(s) = \frac{A(s)\Gamma(s)\bar{L}(s)}{A(s)\Gamma(s)\bar{L}(s) + B(s)P(s)}
\end{equation}

The system response to an output disturbance can then be derived as shown in \eqref{eq:s_imp}.

\begin{equation}
  \label{eq:y_imp}
  Y(s) = S(s)D_o(s) = \frac{S(s)f(o,s)}{\Gamma(s)} = \frac{A(s)\bar{L}(s)}{A(s)\Gamma(s)\bar{L}(s) + B(s)P(s)}f(o,s)
\end{equation}

The inverse Laplace transform $y(t)$ converges to 0 if the controller has been tuned so that all roots to the characteristic polynomial $A(s)\Gamma(s)\bar{L}(s) + B(s)P(s)$ have negative real parts. Hence, the disturbance is asymptotically rejected.

A basic block scheme is shown in Figure~\ref{fig:imp} where $G(s)$ is the system, $C(s) = P(s)/\bar{L}(s)$ the tunable controller, $C_{imp}(s) = 1/\Gamma(s)$ is the compensator and $d_o$ is the considered disturbance.

\begin{figure}[h!]
  \centering %crop: left bottom right top
  \includegraphics[width=0.8\textwidth, trim=6.5cm 5.5cm 5.97cm 11cm, clip=true]{fig/matlab/imp}
  \caption{\label{fig:imp}Block diagram of the \abbrIMP control structure.}
\end{figure}
\newpage
\subsection{Repetitive Feedforward Disturbance Cancellation}
Repetitive control can be used to track and reject periodic disturbances with relatively long periods. For higher frequency modes, it fails to do so due to a number of reasons, but mostly for the inclusion of a low-pass filter, which is needed to maintain stability \citep{fujimoto2009rro}. The conventional repetitive approach uses the \abbrIMP to include a discrete time disturbance model in the feedback controller. However, this approach will make the system more sensitive to other frequencies implying a reduction of the overall tracking capability. With respect to this drawback a novel control scheme with a feedforward switching mechanism and an observer was introduced by the authors in \citep{fujimoto2004repetitive} for the purpose of head-tracking control in hard disk drives. This method is referred to as \emph{Feedforward disturbance rejection with switching
scheme} in the paper but will in this thesis simply be called \abbrRFDC (Repetitive Feedforward Disturbance Cancellation). A block diagram of the control scheme is presented in Figure~\ref{fig:ffrep}, where $G$ and $C$ represent the system and the feedback controller as before. The output disturbance and the observed and replicated compensation signal are denoted $d_o$ and $d_i$, respectively.

\begin{figure}[h]
  \centering %crop: left bottom right top
  \includegraphics[width=0.95\textwidth, trim=6cm 5.5cm 5.2cm 5.5cm, clip=true]{fig/matlab/ffrep}
  \caption{\label{fig:ffrep}Block diagram of a feedforward switching mechanism including an observer and a feedback controller.}
\end{figure}

This method uses an observer to estimate the states of the disturbance. When the states have converged the switch is turned on for one period $T_d$ of the disturbance. This period is then replicated and used to subtract the disturbance from the input signal as illustrated in the block diagram. The delay constant $d$ and the switching on and off time have to be set in advance, hence $T_d$ must be known. Note that if the disturbance frequency is not a multiple of the sampling frequency $T_s$ then extra care has to be taken when setting the delay and switching times. Multiple periods should preferably be used to get a full number of oscillations within the switching timespan that is switched with a period of $T_s$.

The continuous time system in \citep{fujimoto2004repetitive} is defined with $d_o$ added on the input. External disturbances are better modeled as disturbances added to the system output and therefore this approach has changed the position of $d_o$. However, the observer should still be modeling $d_o$ as if it would be added to the input (see \eqref{eq:sys1}) to maintain cancellation of the harmonics at the input of the system. This assumes that $G$ is linear and that the disturbance is sufficiently described by a sinusoidal, since a sinusoidal passing through a linear system only changes in phase and magnitude.

Using a continuous time state space representation, the system and the disturbance can be described as follows

\begin{subequations}
  \label{eq:sys12}
  \begin{alignat}{2}
    \label{eq:sys1}
    & \mathbf{\dot{x}}(t) = \mathbf{A_cx}(t) + \mathbf{B_c}\big(u(t) + d_o(t)\big) \\
    \label{eq:sys2}
    & y(t) = \mathbf{C_cx}(t)
  \end{alignat}
\end{subequations}

and the disturbance as

\begin{subequations}
  \label{eq:dist12}
  \begin{alignat}{2}
    \label{eq:dist1}
    & \mathbf{\dot{x}}_d(t) = \mathbf{A_dx_d}(t) \\
    \label{eq:dist2}
    & d_o(t) = \mathbf{C_dx_d}(t)
  \end{alignat}
\end{subequations}

where $\mathbf{x}$ and $\mathbf{x_d}$ are the system and disturbance state vectors and $\mathbf{A_c, B_c, A_d, C_c}$ and $\mathbf{C_d}$ are known system and disturbance matrices. The one-sided Laplace transform of $\frac{1}{w}\sin(wt)$ is $1/(s^2 + w^2)$, which yields the state space equations in \eqref{eq:sinm} with zero input.

\begin{equation}
  \label{eq:sinm}
  \mathbf{A_d} =
    \begin{bmatrix}
       0 & 1\\[0.3em]
       -w^2 & 0
     \end{bmatrix}
     \qquad
  \mathbf{C_d} =
    \begin{bmatrix}
        1 & 0\\
    \end{bmatrix}
\end{equation}

The discrete time state space representation is obtained by using \eqref{eq:discr123} from \citep{industrial}

\begin{equation}
  \label{eq:discr123}
  \mathbf{A_z} = e^{\mathbf{A}T_s}  \qquad \mathbf{B_z} = \int_{0}^{Ts} e^{\mathbf{A}T_s}\mathbf{B} dt \qquad \mathbf{C_z} = \mathbf{C}
\end{equation}

yielding the equations in \eqref{eq:discrsys}

\begin{subequations}
  \label{eq:discrsys}
  \begin{alignat}{2}
    & \mathbf{x_{zs}}[n + 1] = \mathbf{A_{zs}x_{zs}}[n] + \mathbf{B_{zs}} \big( u_z[n] + d_{zo}[n] \big)\\
    & y_{zs}[n] = \mathbf{C_{zs}x_{zs}}[n] \\
    & \mathbf{x_{zd}}[n + 1] = \mathbf{A_{zd}x_{zd}}[n]\\
    & d_{zo}[n] = \mathbf{C_{zd}x_{zd}}[n]
  \end{alignat}
\end{subequations}

where the disturbance and input are assumed to be piecewise constant during each sampling period $T_s$.

The disturbance is estimated by an observer which is given as

\begin{equation}
  \label{eq:obs}
  \mathbf{\hat{x}}[n + 1] = \mathbf{A\hat{x}}[n] + \mathbf{B}u[n] + \mathbf{K}(\mathbf{y}[n] - \mathbf{C\hat{x}}[n])
\end{equation}

where $\mathbf{A, B}$ and $\mathbf{C}$ are the augmented system matrices and $\mathbf{K}$ is the observer gain.

\begin{equation}
  \label{eq:augumented}
  \mathbf{A} =
    \begin{bmatrix}
       \mathbf{A_{zs}} & \mathbf{C_{zd}B_{zs}}\\[0.3em]
       \mathbf{0} & \mathbf{A_{zd}}\\
     \end{bmatrix}
     \qquad
  \mathbf{B} =
    \begin{bmatrix}
        \mathbf{B_{zs}}\\
        \mathbf{0}
    \end{bmatrix}
     \qquad
  \mathbf{C} =
    \begin{bmatrix}
        \mathbf{C_{zs}} & \mathbf{0}\\
    \end{bmatrix}
\end{equation}

The observer gain should be tuned (placing the eigenvalues of $\mathbf{A-KC}$) with respect to the trade-off between the convergence rate in the state reconstruction and the sensitivity to measurement noise. An optimal choice of  $\mathbf{K}$ can be calculated by the Kalman filter if the noise intensities are known. By deriving the closed loop system, it is shown in \citep{fujimoto2004repetitive} that the disturbance rejection will be achieved at every sampling point in steady state.

The method can be extended to estimate and reject $n$ harmonics by extending $\mathbf{A_d}$ as shown in \eqref{eq:sinexten}, adding $n$ delay loops for each estimated disturbance and by summing all replicated disturbances i.e. $d_i = \sum_{k=1}^{n} d_k$.

\begin{equation}
  \label{eq:sinexten}
  \mathbf{A_{de}} =
    \diag\bigg(\begin{bmatrix}
      \mathbf{A_{d1}}  &  \mathbf{A_{d2}} & \hdots & \mathbf{A_{dn}} \\
     \end{bmatrix}\bigg)
     \qquad
  \mathbf{C_{de}} =
    \begin{bmatrix}
       \mathbf{C_{d1}}  &  \mathbf{C_{d2}} & \hdots & \mathbf{C_{dn}} \\
    \end{bmatrix}
\end{equation}

To reduce the amount of non-modeled disturbances entering the disturbance estimation, the authors in \citep{fujimoto2004repetitive} suggest a bandpass filter to be added after the replication. This could be any type of bandpass filter but it needs to have zero-phase for the selected frequencies, making the range of applicable filters much more narrow. One example of bandpass filter, with zero phase for the selected frequency $\omega$ is

\begin{equation}
  \label{eq:bandpass}
  BP = k\frac{s^2 + 2\xi_a\omega s + \omega^2}{s^2 + 2\xi_b\omega s + \omega^2}
\end{equation}

which has to be discretized before implementation.
\newpage
\subsection{Beat Effect}\label{subsec:beat}
When cancelling one harmonic with another, one might encounter an oscillating effect in the performance known as the "beat effect".
A beat is an interference pattern that occurs due to constructive and destructive interference between two signals that propagate with slightly different frequencies. The summation of the two signals forms an envelope, oscillating with a frequency of half the difference between the two frequencies \citep{beat:2016}. Assume two signals with amplitude one and frequencies $f_1$ and $f_2$ i.e. $y_1 = \cos(2\pi f_1t)$ and $y_2 = \cos(2\pi f_2t)$. Adding these signals and using trigonometric identities yields

\begin{equation}
  y_1 + y_2 = 2cos\bigg(2\pi\frac{f_1+f_2}{2}t\bigg)cos\bigg(2\pi\frac{f_1-f_2}{2}t\bigg)
  \vspace{0.3cm}
\end{equation}

where the last cosine describes the envelope which oscillates with the frequency $f_{beat}$ shown below.

\begin{equation}
  \label{eq:beat}
  f_{beat} = \frac{f_1-f_2}{2}
\end{equation}
